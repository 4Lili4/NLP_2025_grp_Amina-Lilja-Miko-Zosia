{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ae5d4e-b65e-44af-bc6d-7dad0381fe13",
   "metadata": {},
   "source": [
    "## Annotation Quality\n",
    "After you have finished assignment 3, you can contact a TA to obtain data from another annotator. The data will also be made available on LearnIt at 13:30, but we **STRONGLY** recommend to not look at the data from the annotator before completing your annotation.\n",
    "\n",
    "* a) Calculate the accuracy between you and the other annotator, how often did you agree?\n",
    "* b) Now implement Cohen’s Kappa score, and calculate the Kappa for your annotation sample. In which range\n",
    "does your Kappa score fall?\n",
    "* c) Take a closer look at the cases where you disagreed with the other annotator; are these disagreements due\n",
    "to ambiguity, or are there mistakes in the annotation? Would you classify your agreement in the same category as it falls in the standard kappa interpretation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71caf955-c2cd-4b79-85e2-528c71ba61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the second column (labels) from annotated files\n",
    "def extract_labels(file_path):\n",
    "    labels = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f.readlines(): \n",
    "            if line.strip(): # Skipping empty lines or lines without labels\n",
    "                columns = line.split(\"\\t\") # Split the line and get the second column (label)\n",
    "                if len(columns) > 1:\n",
    "                    labels.append(columns[1])  # Second column is the label\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d2c7b-69f6-486e-ba47-979b460ff1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next extract the labels from two files you have after annotation\n",
    "out1_labels = extract_labels(\"pos-data/afek.conllu\") #instead of afek.conllu put the annotateded files\n",
    "out2_labels = extract_labels(\"pos-data/afek.conllu.gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df74f0-59ef-4b2e-a0f5-9b7ea98d99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy between two lists of labels\n",
    "def calculate_accuracy(labels1, labels2):\n",
    "    total_tokens = len(labels1)\n",
    "    correct_tokens = sum(1 for label1, label2 in zip(labels1, labels2) if label1 == label2)\n",
    "    accuracy = correct_tokens / total_tokens\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(out1_labels, out2_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700255b1-baad-48b2-82b5-4dcc2c3c9cb3",
   "metadata": {},
   "source": [
    "### steps:\n",
    "\n",
    "Count how often both annotators assigned the same label.\n",
    "\n",
    "Count how often each annotator assigned each label.\n",
    "\n",
    "Compute the observed agreement and the expected agreement.\n",
    "\n",
    "### Kappa Range:\n",
    "\n",
    "κ=1: Perfect agreement.\n",
    "\n",
    "κ>0.75: Excellent agreement.\n",
    "\n",
    "0.40<κ≤0.75: Fair to good agreement.\n",
    "\n",
    "κ≤0.40: Poor agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae95ae6-f652-43c7-a546-d3ea34714eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4B\n",
    "from collections import Counter\n",
    "\n",
    "def cohen_kappa(annotation1, annotation2):\n",
    "    #how much of each annotation\n",
    "    count1 = Counter(annotation1)\n",
    "    count2 = Counter(annotation2)\n",
    "    \n",
    "    # Observed agreement (P_o)\n",
    "    observed_agreement = sum((a == b) for a, b in zip(annotation1, annotation2)) / len(annotation1)\n",
    "    \n",
    "    # Expected agreement (P_e)\n",
    "    total = len(annotation1)\n",
    "    categories = set(annotation1).union(set(annotation2)) # categories that appear in either of annotations\n",
    "    expected_agreement = 0\n",
    "    \n",
    "    for category in categories:\n",
    "        p1 = count1.get(category, 0) / total\n",
    "        p2 = count2.get(category, 0) / total\n",
    "        expected_agreement += p1 * p2\n",
    "    \n",
    "    # Cohen's Kappa calculation\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement) #from the formula\n",
    "    return kappa\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa = cohen_kappa(out1_labels, out2_labels)\n",
    "print(f\"Cohen's Kappa: {kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b049c-186f-4076-a689-5e89ccd7ed3d",
   "metadata": {},
   "source": [
    "To Interpret Kappa Score:\n",
    "- < 0.2: Poor agreement\n",
    "- 0.2 - 0.4: Fair\n",
    "- 0.4 - 0.6: Moderate\n",
    "- 0.6 - 0.8: Substantial\n",
    "- 0.8 - 1.0: Near-perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24c484-f49f-4cec-a468-43dc163994c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4C\n",
    "def find_disagreements(annotation1, annotation2):\n",
    "    disagreements = []\n",
    "    for idx, (a, b) in enumerate(zip(annotation1, annotation2)):\n",
    "        if a != b:\n",
    "            disagreements.append((idx, a, b))\n",
    "    return disagreements\n",
    "\n",
    "disagreements = find_disagreements(out1_labels, out2_labels)\n",
    "\n",
    "# here I only print the labels that were differing, you can also print out both words and labels \n",
    "# by reading in the files again and accounting for both columns \n",
    "for idx, annot1_label, annot2_label in disagreements:\n",
    "    print(f\"Index: {idx}, Annotator 1: {annot1_label}, Annotator 2: {annot2_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb234b-129d-4197-a7ae-5e9108c61e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
