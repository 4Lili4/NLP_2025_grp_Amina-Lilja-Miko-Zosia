{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea918d3-bb57-42cd-b7ce-6f5f3a202a7b",
   "metadata": {},
   "source": [
    "# Baseline for EWT\n",
    "To get your project started, you start with implementing a baseline model. Ideally, this is going to be the main baseline that you are going to compare to in your paper. Note that this baseline should be more advanced than just predicting the majority class (O).\n",
    "\n",
    "We will use EWT portion of the Universal NER project, which we provide in the folder \"Project_description\" for convenience. You can use the train data (en_ewt-ud-train.iob2) and test data(en_ewt-ud-dev.iob2) to build your baseline, then upload your prediction on the test data (en_ewt-ud-test.iob2).\n",
    "\n",
    "It is important to upload your predictions in same format as the training and test files, so that the span_f1.py script can be used.\n",
    "\n",
    "Note that you do not have to implement your baseline from scratch, you can use for example the code from the RNN or BERT assignments as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb1bff-e24c-4442-b5f3-b0e9d07420ea",
   "metadata": {},
   "source": [
    "__________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e8b76-ba79-43c0-98db-05ebecbf6429",
   "metadata": {},
   "source": [
    "# Homemade baseline implementation with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cae01-bbd3-465b-aecd-9168625916f9",
   "metadata": {},
   "source": [
    "First, import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5b505-a2a8-45ea-9121-f5b19101f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75b570-c89f-4e1d-ad42-6a13c2bddd74",
   "metadata": {},
   "source": [
    "Then, read in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da5e1a-aa74-484b-8088-c5f7325c85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ewt_file(path):\n",
    "    data=[]\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    nr_tags=0\n",
    "    nr_toks=0\n",
    "    \n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line=line.strip()\n",
    "        \n",
    "        if line:\n",
    "            if line[0]=='#':\n",
    "                continue\n",
    "    \n",
    "            elements=line.split('\\t')\n",
    "            nr_toks+=1\n",
    "            \n",
    "            words.append(elements[1])\n",
    "            tags.append(elements[2])\n",
    "            \n",
    "            if elements[3]!='-':\n",
    "                print(elements[3])\n",
    "            if elements[4]=='stephen':\n",
    "                nr_tags+=1\n",
    "    \n",
    "        else:\n",
    "            if words:\n",
    "                data.append((words, tags))\n",
    "            words=[]\n",
    "            tags=[]\n",
    "\n",
    "    if tags!=[]:\n",
    "        data.append((words, tags))\n",
    "\n",
    "    proportion_tagged=nr_tags/nr_toks\n",
    "    \n",
    "    return data, proportion_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d416d0a-756f-4252-bc9c-3cd89d382966",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,prop_tag_train=read_ewt_file('Project_description/en_ewt-ud-train.iob2')\n",
    "print(\"Proportion of training data tagged: \", prop_tag_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15905c34-a5c6-4b11-ab62-6557fbfe39b2",
   "metadata": {},
   "source": [
    "## Vocab and tensor creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d564f-f899-440f-9337-4982d23d1aed",
   "metadata": {},
   "source": [
    "Append each new word and label we meet in the training data to vocab lists, so we can look up indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ebca8-5dac-4e36-9b82-0899e9ca9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a list for each vocabulary, where item at index 0 should just be pad/unknown\n",
    "word_vocab=['<PAD>']\n",
    "label_vocab=['<PAD>']\n",
    "\n",
    "#Iterate over each sentence and corresponding label in the training data\n",
    "for pair in train_data:\n",
    "    #Unpack the tokens and labels, to iterate simultaneously over each word/token and it's label\n",
    "    for word, label in zip(pair[0], pair[1]):\n",
    "\n",
    "        #Check if the word and token already exists in the vocabulary, and if not, add it\n",
    "        if word not in word_vocab:\n",
    "            word_vocab.append(word)\n",
    "        if label not in label_vocab:\n",
    "            label_vocab.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d07cac-d7e7-454f-b348-3276c644dbd4",
   "metadata": {},
   "source": [
    "Save the metrics we need for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d0823-35c0-418a-885a-97bb38155e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_diff_words=len(word_vocab)\n",
    "\n",
    "#Will be used as the nr of rows in our pytorch tensors\n",
    "nr_sent=len(train_data)\n",
    "\n",
    "#Will be used as the nr of columns in our pytorch tensors\n",
    "longest_sent=max([len(x[0]) for x in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766172d-ec1b-4c9b-977d-c1593ef4cf44",
   "metadata": {},
   "source": [
    "Convert each word to it's corresponding vocabulary index, and place it into a pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b5515-477c-4915-87db-7f246bb9979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize tensors with 0-values, this is especially good since the 0-index in our vocabs is just the unknown/pad token and label.\n",
    "# So anything that doesn't get replaced by a vocabulary index, will just have index for <PAD>\n",
    "\n",
    "train_data_matrix=torch.zeros((nr_sent,longest_sent)) #PyTorch tensor of dim 12543 x 159 . Should consist of sentences word by word as rows, and padding for shorter sentences.\n",
    "train_label_matrix=torch.zeros((nr_sent,longest_sent)) #PyTorch tensor of dim 12543 x 159, containing values from the label index for each word in the train_data_matrix\n",
    "\n",
    "#Iterate over the training data again, this time looking up the vocab index for each token and label, to create pytorch tensors of sentence representation\n",
    "for sent_nr, (sentence, labels) in enumerate(train_data):\n",
    "    for tok_nr, (token, label) in enumerate(zip(sentence, labels)):\n",
    "        token_idx=word_vocab.index(token)\n",
    "        label_idx=label_vocab.index(label)\n",
    "        train_data_matrix[sent_nr,tok_nr]=token_idx\n",
    "        train_label_matrix[sent_nr,tok_nr]=label_idx\n",
    "\n",
    "#Convert all matrix values to dType LongInt, since initially adding them to the tensor interpreted the values as float\n",
    "train_data_matrix=train_data_matrix.long()\n",
    "train_label_matrix=train_label_matrix.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c7f94-6cd4-4891-a792-69949d7113c4",
   "metadata": {},
   "source": [
    "________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b517bf-2763-41f7-843f-3491c8380b43",
   "metadata": {},
   "source": [
    "## Now use the pytorch tensors as training data for an RNN\n",
    "#### Okay i lied above, the actual batch-dividing and RNN uses Rob's solution, but the data conversion and prediction output is all our own work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c28e4c-1988-49b9-9bfd-06d5289e8a5c",
   "metadata": {},
   "source": [
    "Below this, we don't 100% understand what's going on, but have adapted Rob's code to make it work for our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2a759-d9dc-4dd2-9daa-51f8d1f8d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feats=torch.zeros((200,100))\n",
    "\n",
    "batch_size=32\n",
    "num_batches=int(len(tmp_feats)/batch_size)\n",
    "\n",
    "tmp_feats_batches=tmp_feats[:batch_size*num_batches].view(num_batches, batch_size, 100)\n",
    "\n",
    "for feats_batch in tmp_feats_batches:\n",
    "    print(feats_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a88cd-625c-4c51-8269-f63d09e7992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(batch_size, train_data_matrix, train_label_matrix):\n",
    "    num_batches=int(len(train_data_matrix)/batch_size)\n",
    "    batches_X=train_data_matrix[:batch_size*num_batches].view(num_batches, batch_size, train_data_matrix.shape[1]) #6, 32, 159\n",
    "    batches_Y=train_label_matrix[:batch_size*num_batches].view(num_batches, batch_size, train_label_matrix.shape[1]) #6, 32, 159\n",
    "    return batches_X, batches_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19bf29-5527-4959-8f5c-06b3177840c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "DIM_EMBEDDING=100\n",
    "RNN_HIDDEN=50\n",
    "BATCH_SIZE=32\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS=10\n",
    "\n",
    "class TaggerModel(torch.nn.Module):\n",
    "    def __init__(self, nwords, ntags):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(nwords, DIM_EMBEDDING)\n",
    "        self.rnn = nn.RNN(DIM_EMBEDDING, RNN_HIDDEN, batch_first=True)\n",
    "        self.fc = nn.Linear(RNN_HIDDEN, ntags)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        word_vectors = self.embed(input_data)\n",
    "        output, hidden = self.rnn(word_vectors)\n",
    "        predictions = self.fc(output)\n",
    "\n",
    "        return predictions \n",
    "\n",
    "model = TaggerModel(len(word_vocab), len(label_vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "#creating the batches \n",
    "batches_X, batches_y = create_batches(BATCH_SIZE, train_data_matrix, train_label_matrix)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    # reset the gradient\n",
    "    model.zero_grad()\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    loss_sum = 0\n",
    "\n",
    "    # loop over batches\n",
    "    for X, y in zip(batches_X, batches_y):\n",
    "        predicted_values = model.forward(X)\n",
    "        predicted_values=predicted_values.view(BATCH_SIZE*longest_sent, -1) #resizing tensor to 2D from 3D\n",
    "        \n",
    "        # calculate loss\n",
    "        y=torch.flatten(y.view(BATCH_SIZE*longest_sent, -1)) #flattening to make it 1D\n",
    "        loss = loss_function(predicted_values, y)\n",
    "        loss_sum += loss.item() #avg later\n",
    "\n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Average loss after epoch {epoch+1}: {loss_sum/batches_X.shape[0]}\")\n",
    "        \n",
    "# set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566852a-a3a7-44f1-8b1b-ba9ec23e9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the test data with the same function as for the training data\n",
    "test_data,prop_tag_test=read_ewt_file('Project_description/en_ewt-ud-dev.iob2')\n",
    "print(\"Proportion of testing data tagged: \", prop_tag_test)\n",
    "\n",
    "#Get the nr of sentences from the testing data, so we can initialize a properly sized pytorch tensor\n",
    "test_nr_sent=len(test_data)\n",
    "test_data_matrix=torch.zeros((test_nr_sent,longest_sent)) #PyTorch tensor of dim 12543 x 159 . Should consist of sentences word by word as rows, and padding for shorter sentences.\n",
    "\n",
    "#Iterate over the testing data, and add each word to the test_data_matrix based on index. We do not need a test_label matrix, as we are trying to predict labels\n",
    "for sent_nr, (sentence, labels) in enumerate(test_data):\n",
    "    for tok_nr, (token, label) in enumerate(zip(sentence, labels)):\n",
    "        #Note that since we are using the word vocabulary created by the training data, we don't know that all words from the testing data will be present\n",
    "        try:\n",
    "            token_idx=word_vocab.index(token)\n",
    "        #New words should be classified as unknown, and have vocab index 0\n",
    "        except:\n",
    "            token_idx=0\n",
    "            \n",
    "        test_data_matrix[sent_nr,tok_nr]=token_idx\n",
    "test_data_matrix=test_data_matrix.long()\n",
    "\n",
    "#Now that our test-data is in the correct format, we run it through our finalized model, to get predicted labels for the test-data\n",
    "predictions_test = model.forward(test_data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26fa294-f83c-4e98-a61a-b93748f1e264",
   "metadata": {},
   "source": [
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d3686-dbc6-4110-bca3-4f364095af6c",
   "metadata": {},
   "source": [
    "## Lastly, from the predictions for each label, take the most likely, and convert to iob2-format for the output file\n",
    "#### Oh yeah, and we're back to homemade solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6055434-0a81-4d4b-89e2-3ad60456df48",
   "metadata": {},
   "source": [
    "First, get the predictions as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e34c03-da2d-4697-a14b-4bea724689ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the predictions_test is a 3-dimensional tensor (8 layers of 2d-matrices, with each layer representing a possible label), \n",
    "# we need to get the most likely label for each token, and look that index up in our label_vocab for the word-label\n",
    "test_pred=[]\n",
    "for i, sentence in enumerate(predictions_test[:,:,]):\n",
    "    labels=[]\n",
    "    for j, token in enumerate(sentence):\n",
    "        label=label_vocab[torch.argmax(predictions_test[i,j,:])]\n",
    "        labels.append(label)\n",
    "    test_pred.append((test_data[i][0],labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db59c72-5142-4ee5-8b7c-67d1b14dd8e1",
   "metadata": {},
   "source": [
    "Then, assemble it all into a string of the proper format - with 5 columns of nr, token, label, \"-\", and \"stephen\" if the token has a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a6e7b-dd28-4c34-a010-c5f8de14f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get it as iob2-format output, we assemble the predicted labels with the corresponding words in a string, as well as a \"stephen\" if a word has a label\n",
    "output_txt=\"\"\n",
    "for sentence, labels in test_pred:\n",
    "    output_txt+=\"\\n# text = \"+\" \".join(sentence)+\"\\n\"\n",
    "    for i, (token, label) in enumerate(zip(sentence,labels)):\n",
    "        steph=\"-\"\n",
    "        if label != \"O\":\n",
    "            steph=\"stephen\"\n",
    "        line=str(i+1)+\"\\t\"+token+\"\\t\"+label+\"\\t-\\t\"+steph+\"\\n\"\n",
    "        output_txt+=line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832eb2ae-6167-4515-8575-1ee7455b6265",
   "metadata": {},
   "source": [
    "Very last step: Save the string as a new file in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f182e-009c-44ee-813a-296b9e2fd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, write that string into a file\n",
    "with open(\"baseline_test_pred_output.iob2\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(output_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f6a40-3e16-4085-bc0f-e21f2b9530dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
