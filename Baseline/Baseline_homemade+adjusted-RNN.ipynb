{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5e8b76-ba79-43c0-98db-05ebecbf6429",
   "metadata": {},
   "source": [
    "## Baseline implementation with RNN, a mix of homemade code, and solutions given from ass.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e100d-299d-45a6-be70-0c9c5f39d6fc",
   "metadata": {},
   "source": [
    "Reference: Rob's solution to assignment 4\n",
    "\n",
    "**Adjustable stuffs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd3aaa2-fe76-4705-bfad-ef35b65ec935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input and output paths \n",
    "training_data_path = 'Project_description/en_ewt-ud-train.iob2'\n",
    "validation_data_path = 'Project_description/en_ewt-ud-dev.iob2'\n",
    "testing_data_path = 'Project_description/en_ewt-ud-test-masked.iob2'\n",
    "\n",
    "output_predictions_path_and_name = 'baseline_test_pred_output.iob2'\n",
    "\n",
    "#Model hyperparameters\n",
    "batch_size=32\n",
    "nr_embedding_dimensions=100\n",
    "nr_hidden_rnn_layers=50\n",
    "learning_rate=0.01\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea918d3-bb57-42cd-b7ce-6f5f3a202a7b",
   "metadata": {},
   "source": [
    "# From the project description: Baseline for EWT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d3ca5-cc89-4b04-af39-e49bc6779bdc",
   "metadata": {},
   "source": [
    "To get your project started, you start with implementing a baseline model. Ideally, this is going to be the main baseline that you are going to compare to in your paper. Note that this baseline should be more advanced than just predicting the majority class (O).\n",
    "\n",
    "We will use EWT portion of the Universal NER project, which we provide in the folder \"Project_description\" for convenience. You can use the train data (en_ewt-ud-train.iob2) and test data(en_ewt-ud-dev.iob2) to build your baseline, then upload your prediction on the test data (en_ewt-ud-test.iob2).\n",
    "\n",
    "It is important to upload your predictions in same format as the training and test files, so that the span_f1.py script can be used.\n",
    "\n",
    "Note that you do not have to implement your baseline from scratch, you can use for example the code from the RNN or BERT assignments as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cae01-bbd3-465b-aecd-9168625916f9",
   "metadata": {},
   "source": [
    "# First, import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f5b505-a2a8-45ea-9121-f5b19101f4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x207fbe97f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75b570-c89f-4e1d-ad42-6a13c2bddd74",
   "metadata": {},
   "source": [
    "# All the functions are gathered below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c78b20-4f9f-44fb-ab80-0566ea25b739",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "Homemade code, inspired by Rob's conll-implementation to ensure that we remembered all checks and outlying cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44da5e1a-aa74-484b-8088-c5f7325c85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob_file(path):\n",
    "    data=[]\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    nr_tags=0\n",
    "    nr_toks=0\n",
    "\n",
    "    data_file_name=path.split(\"/\")[-1]\n",
    "    \n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line=line.strip()\n",
    "\n",
    "        #Start with a check to filter out empty lines and comments\n",
    "        if line !=\"\" and line[0]!=\"#\":\n",
    "            elements=line.split('\\t')\n",
    "            nr_toks+=1\n",
    "            \n",
    "            words.append(elements[1])\n",
    "            tags.append(elements[2])\n",
    "\n",
    "            #We are interested in how much of the data is actually tagged as an entity\n",
    "            if elements[4]=='stephen':\n",
    "                nr_tags+=1\n",
    "\n",
    "        #If we do reach an empty line or sudden comment, then the current sentence has ended, and we append all the stored tokens and labels to a list of gathered data\n",
    "        else:\n",
    "            if words!=[]:\n",
    "                data.append((words, tags))\n",
    "            words=[]\n",
    "            tags=[]\n",
    "\n",
    "    if tags!=[]:\n",
    "        data.append((words, tags))\n",
    "\n",
    "    proportion_tagged=nr_tags/nr_toks\n",
    "    print(f\"Proportion of {data_file_name} data with a tag: \", proportion_tagged)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef6746-cbb6-418c-8f50-0c852991ce62",
   "metadata": {},
   "source": [
    "________________________________________\n",
    "Homemade code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95496203-a70e-4927-b075-22e6b8a2bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabs(data_list):\n",
    "    words=['<PAD>']\n",
    "    labels=['<PAD>']\n",
    "    \n",
    "    #Iterate over each sentence and corresponding label in the training data\n",
    "    for pair in data_list:\n",
    "        #Unpack the tokens and labels, to iterate simultaneously over each word/token and it's label\n",
    "        for word, label in zip(pair[0], pair[1]):\n",
    "    \n",
    "            #Check if the word and token already exists in the vocabulary, and if not, add it\n",
    "            if word not in words:\n",
    "                words.append(word)\n",
    "            if label not in labels:\n",
    "                labels.append(label)\n",
    "\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4b38e-fcaf-4076-adda-3d1f76cf508e",
   "metadata": {},
   "source": [
    "________________________________________\n",
    "Homemade code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06aa723-7014-419b-bbef-8ae59cca0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2matrix(data_list, tok_vocab, lab_vocab):\n",
    "    \n",
    "    nr_sent=len(data_list)\n",
    "    longest_sent=max([len(x[0]) for x in train_data])\n",
    "    \n",
    "    data_matrix=torch.zeros((nr_sent,longest_sent)) #PyTorch tensor of dim 12543 x 159 . Should consist of sentences word by word as rows, and padding for shorter sentences.\n",
    "    label_matrix=torch.zeros((nr_sent,longest_sent)) #PyTorch tensor of dim 12543 x 159, containing values from the label index for each word in the train_data_matrix\n",
    "    \n",
    "    #Iterate over the training data again, this time looking up the vocab index for each token and label, to create pytorch tensors of sentence representation\n",
    "    for sent_nr, (sentence, labels) in enumerate(data_list):\n",
    "        for tok_nr, (token, label) in enumerate(zip(sentence, labels)):\n",
    "\n",
    "            try:\n",
    "                token_idx=word_vocab.index(token)\n",
    "            #New words occuring in the testing data should be classified as unknown, and have vocab index 0\n",
    "            except:\n",
    "                token_idx=0\n",
    "                \n",
    "            label_idx=label_vocab.index(label)\n",
    "            \n",
    "            data_matrix[sent_nr,tok_nr]=token_idx\n",
    "            label_matrix[sent_nr,tok_nr]=label_idx\n",
    "    \n",
    "    #Convert all matrix values to dType LongInt, since initially adding them to the tensor interpreted the values as float\n",
    "    data_matrix=data_matrix.long()\n",
    "    label_matrix=label_matrix.long()\n",
    "\n",
    "    return data_matrix, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ba401-2e3a-4a86-a14b-00d9e259e793",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "Rob's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64171755-0364-47c8-af41-f4c3e014908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(batch_size, train_data_matrix, train_label_matrix):\n",
    "    num_batches=int(len(train_data_matrix)/batch_size)\n",
    "    batches_X=train_data_matrix[:batch_size*num_batches].view(num_batches, batch_size, train_data_matrix.shape[1]) #6, 32, 159\n",
    "    batches_Y=train_label_matrix[:batch_size*num_batches].view(num_batches, batch_size, train_label_matrix.shape[1]) #6, 32, 159\n",
    "    return batches_X, batches_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958e9d1-0f4f-4a5a-b477-4b08171b2a57",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "Rob's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f9d1b2-475a-4d46-9897-53a311f73580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggerModel(torch.nn.Module):\n",
    "    def __init__(self, nwords, ntags):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(nwords, nr_embedding_dimensions)\n",
    "        self.rnn = nn.RNN(nr_embedding_dimensions, nr_hidden_rnn_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(nr_hidden_rnn_layers, ntags)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        word_vectors = self.embed(input_data)\n",
    "        output, hidden = self.rnn(word_vectors)\n",
    "        predictions = self.fc(output)\n",
    "\n",
    "        return predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f9098-8df7-45c6-8479-95cf6f387475",
   "metadata": {},
   "source": [
    "_______________________________________\n",
    "Homemade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626afc6f-df52-4b69-8674-129415dd59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the predictions_test is a 3-dimensional tensor (8 layers of 2d-matrices, with each layer representing a possible label), \n",
    "# we need to get the most likely label for each token, and look that index up in our label_vocab for the word-label\n",
    "def get_predictions(plain_data, likelihood_3d_tensor):\n",
    "    \n",
    "    predictions=[]\n",
    "    \n",
    "    for i, sentence in enumerate(likelihood_3d_tensor[:,:,]):\n",
    "        \n",
    "        labels=[]\n",
    "        \n",
    "        for j, token in enumerate(sentence):\n",
    "            \n",
    "            label = label_vocab[torch.argmax(likelihood_3d_tensor[i,j,:])]\n",
    "            labels.append(label)\n",
    "            \n",
    "        predictions.append((plain_data[i][0],labels))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5fb970-314a-4410-b831-48d1075caf1f",
   "metadata": {},
   "source": [
    "___________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec914201-4cfe-4fbd-b99a-4c3770c7d7bf",
   "metadata": {},
   "source": [
    "# Main code for model creation from training data below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b640ed5-f85d-4f09-8fdc-81029c86197a",
   "metadata": {},
   "source": [
    "### Reading in training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d416d0a-756f-4252-bc9c-3cd89d382966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of en_ewt-ud-train.iob2 data with a tag:  0.050640583833140254\n"
     ]
    }
   ],
   "source": [
    "train_data=read_iob_file(training_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15905c34-a5c6-4b11-ab62-6557fbfe39b2",
   "metadata": {},
   "source": [
    "### Vocab and tensor creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b473b80-6d55-4b22-aacf-a9840b720621",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab, label_vocab = create_vocabs(train_data)\n",
    "train_data_matrix, train_label_matrix=conv2matrix(train_data, word_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c28e4c-1988-49b9-9bfd-06d5289e8a5c",
   "metadata": {},
   "source": [
    "### Dividing the training data into batches\n",
    "The code below is taken from Rob's solutions to assignment 4, but has been adjusted to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58c2a759-d9dc-4dd2-9daa-51f8d1f8d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_feats=torch.zeros((200,100))\n",
    "\n",
    "num_batches=int(len(tmp_feats)/batch_size)\n",
    "\n",
    "tmp_feats_batches=tmp_feats[:batch_size*num_batches].view(num_batches, batch_size, 100)\n",
    "\n",
    "#creating the batches \n",
    "word_batches, label_batches = create_batches(batch_size, train_data_matrix, train_label_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c076051-7851-4ef0-84b9-296fcfa5d47d",
   "metadata": {},
   "source": [
    "### Model creation and adjusting\n",
    "The code below is taken from Rob's solutions to assignment 4, but has been adjusted to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae19bf29-5527-4959-8f5c-06b3177840c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Average loss after epoch 1: 111.6510002625263\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Average loss after epoch 2: 45.17123385837011\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Average loss after epoch 3: 23.7831861356945\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Average loss after epoch 4: 15.608368954761788\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Average loss after epoch 5: 11.644263831874751\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Average loss after epoch 6: 9.87129383909104\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Average loss after epoch 7: 8.698806805074062\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Average loss after epoch 8: 8.140867290127536\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Average loss after epoch 9: 7.39918151096729\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Average loss after epoch 10: 7.678291720816928\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaggerModel(\n",
       "  (embed): Embedding(19674, 100)\n",
       "  (rnn): RNN(100, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TaggerModel(len(word_vocab), len(label_vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    model.train()\n",
    "    # reset the gradient\n",
    "    model.zero_grad()\n",
    "    print(f\"Epoch {iteration+1}\\n-------------------------------\")\n",
    "    loss_sum = 0\n",
    "\n",
    "    # loop over batches\n",
    "    for x, y in zip(word_batches, label_batches):\n",
    "        predicted_values = model.forward(x)\n",
    "        predicted_values=predicted_values.view(batch_size*train_data_matrix.shape[1], -1) #resizing tensor to 2D from 3D\n",
    "        \n",
    "        # calculate loss\n",
    "        y=torch.flatten(y.view(batch_size*train_data_matrix.shape[1], -1)) #flattening to make it 1D\n",
    "        loss = loss_function(predicted_values, y)\n",
    "        loss_sum += loss.item() #avg later\n",
    "\n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Average loss after epoch {iteration+1}: {loss_sum/word_batches.shape[0]}\\n\")\n",
    "        \n",
    "# set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17348f95-55f0-4310-ba1e-35710e3a4095",
   "metadata": {},
   "source": [
    "### Validation of the model on the dev data done according to Rob's solutions to ass.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b44d7eb-2344-49c3-9d8b-ee98f5a2eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of en_ewt-ud-dev.iob2 data with a tag:  0.05948546661895105\n"
     ]
    }
   ],
   "source": [
    "val_data = read_iob_file(validation_data_path)\n",
    "val_data_matrix, val_label_matrix = conv2matrix(val_data, word_vocab, label_vocab)\n",
    "val_predictions = model.forward(val_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46a509e8-a4b0-4c52-8ec8-660a064a4e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on validation set: 0.9564197383593781\n"
     ]
    }
   ],
   "source": [
    "labels_val = torch.argmax(val_predictions, 2)\n",
    "labels_val = torch.flatten(labels_val) #model predictions\n",
    "dev_y_flat = torch.flatten(val_label_matrix) #true labels\n",
    "acc = []\n",
    "for i in range(len(labels_val)):\n",
    "    if dev_y_flat[i]!=0:\n",
    "        acc.append(int(labels_val[i]==dev_y_flat[i]))\n",
    "\n",
    "accuracy = sum(acc)/len(acc)\n",
    "print(f\"Model accuracy on validation set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518812d7-b8e4-425c-a9a6-d2b858d670c5",
   "metadata": {},
   "source": [
    "# Running our created model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5049be9-411d-4f68-a09d-b30adf5ee7be",
   "metadata": {},
   "source": [
    "### Read in data and convert it to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2ac336e-1972-45b7-8474-79afcaff400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of en_ewt-ud-test-masked.iob2 data with a tag:  0.06690042634577838\n"
     ]
    }
   ],
   "source": [
    "test_data = read_iob_file(testing_data_path)\n",
    "test_data_matrix, _ = conv2matrix(test_data, word_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de60079-c453-4a74-a5a1-a12a22277499",
   "metadata": {},
   "source": [
    "### Run the test data through the model to get 3-dimensional likelihood distribution of labels, and get predictions from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd1c011-c1d0-49b9-81ef-29c27f05ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.forward(test_data_matrix)\n",
    "test_pred = get_predictions(test_data, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d89fa3-c681-4222-aa01-4890d9cf07ec",
   "metadata": {},
   "source": [
    "### Now convert to iob2-format for the output file, and save it as a new file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db59c72-5142-4ee5-8b7c-67d1b14dd8e1",
   "metadata": {},
   "source": [
    "Assemble it all into a string of the proper format: with 5 columns of [ nr, token, label, \"-\", and \"stephen\" (if the token has a label) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a62a6e7b-dd28-4c34-a010-c5f8de14f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get it as iob2-format output, we assemble the predicted labels with the corresponding words in a string, as well as a \"stephen\" if a word has a label\n",
    "output_txt=\"\"\n",
    "for sentence, labels in test_pred:\n",
    "    output_txt+=\"\\n# text = \"+\" \".join(sentence)+\"\\n\"\n",
    "    for i, (token, label) in enumerate(zip(sentence,labels)):\n",
    "        steph=\"-\"\n",
    "        if label != \"O\":\n",
    "            steph=\"stephen\"\n",
    "        line=str(i+1)+\"\\t\"+token+\"\\t\"+label+\"\\t-\\t\"+steph+\"\\n\"\n",
    "        output_txt+=line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "714f182e-009c-44ee-813a-296b9e2fd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, write that string into a file\n",
    "with open(output_predictions_path_and_name, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(output_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
