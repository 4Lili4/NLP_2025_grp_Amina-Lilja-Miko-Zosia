{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daaed996-8a90-4c96-9323-7bfcfbaf04f7",
   "metadata": {},
   "source": [
    "# Baseline for EWT\n",
    "To get your project started, you start with implementing a baseline model. Ideally, this is going to be the main baseline that you are going to compare to in your paper. Note that this baseline should be more advanced than just predicting the majority class (O).\n",
    "\n",
    "We will use EWT portion of the Universal NER project, which we provide in the folder \"Project_description\" for convenience. You can use the train data (en_ewt-ud-train.iob2) and test data(en_ewt-ud-dev.iob2) to build your baseline, then upload your prediction on the test data (en_ewt-ud-test.iob2).\n",
    "\n",
    "It is important to upload your predictions in same format as the training and test files, so that the span_f1.py script can be used.\n",
    "\n",
    "Note that you do not have to implement your baseline from scratch, you can use for example the code from the RNN or BERT assignments as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691647c-77cd-4c78-8100-72f35e6e838c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1: read in the data\n",
    "Conll-approach retro-fitted for ewt-data, adjusted to be iob2-format-friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b813eb5c-0266-4f4a-a8ab-7690fc6538a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ewt_file(path):\n",
    "    data=[]\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    nr_tags=0\n",
    "    nr_toks=0\n",
    "    \n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line=line.strip()\n",
    "        \n",
    "        if line:\n",
    "            if line[0]=='#':\n",
    "                continue\n",
    "    \n",
    "            elements=line.split('\\t')\n",
    "            nr_toks+=1\n",
    "            \n",
    "            words.append(elements[1])\n",
    "            tags.append(elements[2])\n",
    "            \n",
    "            if elements[3]!='-':\n",
    "                print(elements[3])\n",
    "            if elements[4]=='stephen':\n",
    "                nr_tags+=1\n",
    "    \n",
    "        else:\n",
    "            if words:\n",
    "                data.append((words, tags))\n",
    "            words=[]\n",
    "            tags=[]\n",
    "\n",
    "    if tags!=[]:\n",
    "        data.append((words, tags))\n",
    "\n",
    "    proportion_tagged=nr_tags/nr_toks\n",
    "    \n",
    "    return data, proportion_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c66c-2c03-4ab3-ba2a-24ac7e517bda",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7846e188-d6eb-4d0b-98cf-3202c149018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of training data tagged:  0.050640583833140254\n",
      "Proportion of testing data tagged:  0.05948546661895105\n"
     ]
    }
   ],
   "source": [
    "train_data,prop_tag_train=read_ewt_file('Project_description/en_ewt-ud-train.iob2')\n",
    "test_data,prop_tag_test=read_ewt_file('Project_description/en_ewt-ud-dev.iob2')\n",
    "print(\"Proportion of training data tagged: \", prop_tag_train)\n",
    "print(\"Proportion of testing data tagged: \", prop_tag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1782744-4e8a-4543-9ea8-d932117d0a5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 2.1: Implement RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f24a14c-8efb-462b-b9f5-eeabed5f2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, pad_unk='<PAD>'):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word(idx)\n",
    "\n",
    "max_len= max([len(x[0]) for x in train_data ])\n",
    "\n",
    "class preprocess():\n",
    "    \"\"\"\n",
    "    data: the dataset from which we get the matrix used by a Neural network (instances + their tags)\n",
    "    instances: number of instances in the dataset, needed for dimension of matrix\n",
    "    features: the number of features/columns of the matrix\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_words = Vocab()\n",
    "        self.vocab_tags = Vocab()\n",
    "\n",
    "    def build_vocab(self, data, instances, features):\n",
    "        data_X = torch.zeros(instances, features, dtype=int)\n",
    "        data_y = torch.zeros(instances, features, dtype=int)\n",
    "        for i, sentence_tags in enumerate(data):\n",
    "            for j, word in enumerate(sentence_tags[0]):\n",
    "                data_X[i, j]=self.vocab_words.getIdx(word=word, add=True)\n",
    "                data_y[i, j]=self.vocab_tags.getIdx(word=sentence_tags[1][j], add=True)\n",
    "\n",
    "        #returns the list of unique words in the list from the attributes of the Vocab() \n",
    "        idx2word_train = self.vocab_words.idx2word\n",
    "        #returns the list of unique tags in the list from the attributes of the Vocab() \n",
    "        idx2label_train = self.vocab_tags.idx2word\n",
    "        #only returned in the builder function, because they are reused for test data in transform_prep_data()\n",
    "        return data_X, data_y, idx2word_train, idx2label_train\n",
    "\n",
    "    def transform_prep_data(self, data, instances, features):\n",
    "        #to be used only on test data\n",
    "        data_X = torch.zeros(instances, features, dtype=int)\n",
    "        data_y = torch.zeros(instances, features, dtype=int)\n",
    "        for i, sentence_tags in enumerate(data):\n",
    "            for j, word in enumerate(sentence_tags[0]):\n",
    "                data_X[i, j]=self.vocab_words.getIdx(word=word, add=False)\n",
    "                data_y[i, j]=self.vocab_tags.getIdx(word=sentence_tags[1][j], add=False)\n",
    "        return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10126bb-a0eb-445d-8198-0b2d5d7f5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12543, 159]) torch.Size([12543, 159])\n",
      "torch.Size([2001, 159]) torch.Size([2001, 159])\n"
     ]
    }
   ],
   "source": [
    "transformer = preprocess()\n",
    "train_X, train_y, idx2word, idx2label = transformer.build_vocab(train_data, len(train_data), max_len)\n",
    "test_X, test_y = transformer.transform_prep_data(test_data, len(test_data), max_len)\n",
    "\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada3648a-8ae6-4585-9851-a5ab4e4d7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([200, 100])\n",
      "torch.Size([6, 32, 100])\n",
      "\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "tmp_feats = torch.zeros((200, 100))\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = int(len(tmp_feats)/batch_size)\n",
    "\n",
    "print(num_batches)\n",
    "\n",
    "print(tmp_feats.shape)\n",
    "\n",
    "tmp_feats_batches = tmp_feats[:batch_size*num_batches].view(num_batches,batch_size, 100)\n",
    "\n",
    "# 6 batches with 32 instances with 100 features\n",
    "print(tmp_feats_batches.shape)\n",
    "\n",
    "print()\n",
    "for feats_batch in tmp_feats_batches:\n",
    "    print(feats_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7efbfc1-84e1-4822-86ae-a0a5dd881e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(batch_size, train_X, train_y):\n",
    "\n",
    "    num_batches = int(len(train_X)/batch_size)\n",
    "\n",
    "    # print(\"Num of batches: \", num_batches)\n",
    "\n",
    "    batches_X = train_X[:batch_size*num_batches].view(num_batches,batch_size, train_X.shape[1])\n",
    "    batches_y = train_y[:batch_size*num_batches].view(num_batches,batch_size, train_y.shape[1])\n",
    "\n",
    "    # batches = torch.cat((batches_X, batches_y), -1)\n",
    "    # print(\"Shape of batches full of X inst: \", batches_X.shape)\n",
    "    # print(\"Shape of batches full of y inst: \", batches_y.shape)\n",
    "    return batches_X, batches_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc547e2a-648e-46dd-92e4-6f35b7e66621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Average loss after epoch 1: 111.21723593348432\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Average loss after epoch 2: 46.148390804105404\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Average loss after epoch 3: 24.38766194639913\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Average loss after epoch 4: 16.037084814189644\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Average loss after epoch 5: 11.571120053177218\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Average loss after epoch 6: 9.950318863620158\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Average loss after epoch 7: 8.601709420676045\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Average loss after epoch 8: 8.87185892321722\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Average loss after epoch 9: 8.495747115825067\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Average loss after epoch 10: 8.317945576689738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaggerModel(\n",
       "  (embed): Embedding(19674, 100)\n",
       "  (rnn): RNN(100, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "torch.manual_seed(0)\n",
    "DIM_EMBEDDING = 100\n",
    "RNN_HIDDEN = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "\n",
    "class TaggerModel(torch.nn.Module):\n",
    "    def __init__(self, nwords, ntags):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        self.embed = nn.Embedding(nwords, DIM_EMBEDDING)\n",
    "        self.rnn = nn.RNN(DIM_EMBEDDING, RNN_HIDDEN, batch_first=True)\n",
    "        self.fc = nn.Linear(RNN_HIDDEN, ntags)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        word_vectors = self.embed(input_data)\n",
    "        output, hidden = self.rnn(word_vectors)\n",
    "        predictions = self.fc(output)\n",
    "\n",
    "        return predictions \n",
    "\n",
    "model = TaggerModel(len(idx2word), len(idx2label))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "#creating the batches \n",
    "batches_X, batches_y = create_batches(BATCH_SIZE, train_X, train_y)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    # reset the gradient\n",
    "    model.zero_grad()\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    loss_sum = 0\n",
    "\n",
    "    # loop over batches\n",
    "    for X, y in zip(batches_X, batches_y):\n",
    "        predicted_values = model.forward(X)\n",
    "        predicted_values=predicted_values.view(BATCH_SIZE*max_len, -1) #resizing tensor to 2D from 3D\n",
    "        \n",
    "        # calculate loss\n",
    "        y=torch.flatten(y.view(BATCH_SIZE*max_len, -1)) #flattening to make it 1D\n",
    "        loss = loss_function(predicted_values, y)\n",
    "        loss_sum += loss.item() #avg later\n",
    "\n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Average loss after epoch {epoch+1}: {loss_sum/batches_X.shape[0]}\")\n",
    "        \n",
    "# set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8bd1d-0165-4105-8f12-4a0f33063bd0",
   "metadata": {},
   "source": [
    "## RNN eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04983a4b-756b-40a1-9a2b-0e6b03414c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2001, 159, 8])\n",
      "Model accuracy on test set: 0.9429400771402442\n"
     ]
    }
   ],
   "source": [
    "#Evaluating on test data we will predict using trained TaggerModel\n",
    "predictions_test = model.forward(test_X)\n",
    "print(predictions_test.shape)\n",
    "#gives probabilities for each tag (dim=18) for each word/feature (dim=159) for each sentence(dim=2000)\n",
    "#we want to classify each word for the part-of-speech with highest probability\n",
    "labels_test = torch.argmax(predictions_test, 2)\n",
    "labels_test.shape\n",
    "\n",
    "labels_test = torch.flatten(labels_test) #model predictions\n",
    "test_y_flat = torch.flatten(test_y) #true labels\n",
    "acc = []\n",
    "for i in range(len(labels_test)):\n",
    "    if test_y_flat[i]!=0:\n",
    "        acc.append(int(labels_test[i]==test_y_flat[i]))\n",
    "\n",
    "accuracy = sum(acc)/len(acc)\n",
    "print(f\"Model accuracy on test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f446c5-3aa1-4714-a718-0c5c1da37288",
   "metadata": {},
   "source": [
    "## Step 2.2: Implement BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f43c6e-bd35-473f-9fa7-5140729e6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input code from solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421f120-f58f-4872-b500-da7d376e3747",
   "metadata": {},
   "source": [
    "## BERT eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3bfc4-08f1-4427-a4c0-9702dca9a3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
