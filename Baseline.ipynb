{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daaed996-8a90-4c96-9323-7bfcfbaf04f7",
   "metadata": {},
   "source": [
    "# Baseline for EWT\n",
    "To get your project started, you start with implementing a baseline model. Ideally, this is going to be the main baseline that you are going to compare to in your paper. Note that this baseline should be more advanced than just predicting the majority class (O).\n",
    "\n",
    "We will use EWT portion of the Universal NER project, which we provide in the folder \"Project_description\" for convenience. You can use the train data (en_ewt-ud-train.iob2) and test data(en_ewt-ud-dev.iob2) to build your baseline, then upload your prediction on the test data (en_ewt-ud-test.iob2).\n",
    "\n",
    "It is important to upload your predictions in same format as the training and test files, so that the span_f1.py script can be used.\n",
    "\n",
    "Note that you do not have to implement your baseline from scratch, you can use for example the code from the RNN or BERT assignments as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691647c-77cd-4c78-8100-72f35e6e838c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Step 1:** read in the data\n",
    "Conll-approach retro-fitted for ewt-data, adjusted to be iob2-format-friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b813eb5c-0266-4f4a-a8ab-7690fc6538a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ewt_file(path):\n",
    "    data=[]\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    nr_tags=0\n",
    "    nr_toks=0\n",
    "    \n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line=line.strip()\n",
    "        \n",
    "        if line:\n",
    "            if line[0]=='#':\n",
    "                continue\n",
    "    \n",
    "            elements=line.split('\\t')\n",
    "            nr_toks+=1\n",
    "            \n",
    "            words.append(elements[1])\n",
    "            tags.append(elements[2])\n",
    "            \n",
    "            if elements[3]!='-':\n",
    "                print(elements[3])\n",
    "            if elements[4]=='stephen':\n",
    "                nr_tags+=1\n",
    "    \n",
    "        else:\n",
    "            if words:\n",
    "                data.append((words, tags))\n",
    "            words=[]\n",
    "            tags=[]\n",
    "\n",
    "    if tags!=[]:\n",
    "        data.append((words, tags))\n",
    "\n",
    "    proportion_tagged=nr_tags/nr_toks\n",
    "    \n",
    "    return data, proportion_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c66c-2c03-4ab3-ba2a-24ac7e517bda",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7846e188-d6eb-4d0b-98cf-3202c149018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of training data tagged:  0.050640583833140254\n",
      "Proportion of testing data tagged:  0.05948546661895105\n"
     ]
    }
   ],
   "source": [
    "train_data,prop_tag_train=read_ewt_file('Project_description/en_ewt-ud-train.iob2')\n",
    "test_data,prop_tag_test=read_ewt_file('Project_description/en_ewt-ud-dev.iob2')\n",
    "print(\"Proportion of training data tagged: \", prop_tag_train)\n",
    "print(\"Proportion of testing data tagged: \", prop_tag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1782744-4e8a-4543-9ea8-d932117d0a5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Step 2.1:** Implement RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f24a14c-8efb-462b-b9f5-eeabed5f2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, pad_unk='<PAD>'):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word(idx)\n",
    "\n",
    "max_len= max([len(x[0]) for x in train_data ])\n",
    "\n",
    "class preprocess():\n",
    "    \"\"\"\n",
    "    data: the dataset from which we get the matrix used by a Neural network (instances + their tags)\n",
    "    instances: number of instances in the dataset, needed for dimension of matrix\n",
    "    features: the number of features/columns of the matrix\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_words = Vocab()\n",
    "        self.vocab_tags = Vocab()\n",
    "\n",
    "    def build_vocab(self, data, instances, features):\n",
    "        data_X = torch.zeros(instances, features, dtype=int)\n",
    "        data_y = torch.zeros(instances, features, dtype=int)\n",
    "        for i, sentence_tags in enumerate(data):\n",
    "            for j, word in enumerate(sentence_tags[0]):\n",
    "                data_X[i, j]=self.vocab_words.getIdx(word=word, add=True)\n",
    "                data_y[i, j]=self.vocab_tags.getIdx(word=sentence_tags[1][j], add=True)\n",
    "\n",
    "        #returns the list of unique words in the list from the attributes of the Vocab() \n",
    "        idx2word_train = self.vocab_words.idx2word\n",
    "        #returns the list of unique tags in the list from the attributes of the Vocab() \n",
    "        idx2label_train = self.vocab_tags.idx2word\n",
    "        #only returned in the builder function, because they are reused for test data in transform_prep_data()\n",
    "        return data_X, data_y, idx2word_train, idx2label_train\n",
    "\n",
    "    def transform_prep_data(self, data, instances, features):\n",
    "        #to be used only on test data\n",
    "        data_X = torch.zeros(instances, features, dtype=int)\n",
    "        data_y = torch.zeros(instances, features, dtype=int)\n",
    "        for i, sentence_tags in enumerate(data):\n",
    "            for j, word in enumerate(sentence_tags[0]):\n",
    "                data_X[i, j]=self.vocab_words.getIdx(word=word, add=False)\n",
    "                data_y[i, j]=self.vocab_tags.getIdx(word=sentence_tags[1][j], add=False)\n",
    "        return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10126bb-a0eb-445d-8198-0b2d5d7f5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12543, 159]) torch.Size([12543, 159])\n",
      "torch.Size([2001, 159]) torch.Size([2001, 159])\n"
     ]
    }
   ],
   "source": [
    "transformer = preprocess()\n",
    "train_X, train_y, idx2word, idx2label = transformer.build_vocab(train_data, len(train_data), max_len)\n",
    "test_X, test_y = transformer.transform_prep_data(test_data, len(test_data), max_len)\n",
    "\n",
    "print(train_X.shape, train_y.shape)\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada3648a-8ae6-4585-9851-a5ab4e4d7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([200, 100])\n",
      "torch.Size([6, 32, 100])\n",
      "\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "tmp_feats = torch.zeros((200, 100))\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = int(len(tmp_feats)/batch_size)\n",
    "\n",
    "print(num_batches)\n",
    "\n",
    "print(tmp_feats.shape)\n",
    "\n",
    "tmp_feats_batches = tmp_feats[:batch_size*num_batches].view(num_batches,batch_size, 100)\n",
    "\n",
    "# 6 batches with 32 instances with 100 features\n",
    "print(tmp_feats_batches.shape)\n",
    "\n",
    "print()\n",
    "for feats_batch in tmp_feats_batches:\n",
    "    print(feats_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7efbfc1-84e1-4822-86ae-a0a5dd881e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(batch_size, train_X, train_y):\n",
    "\n",
    "    num_batches = int(len(train_X)/batch_size)\n",
    "\n",
    "    # print(\"Num of batches: \", num_batches)\n",
    "\n",
    "    batches_X = train_X[:batch_size*num_batches].view(num_batches,batch_size, train_X.shape[1])\n",
    "    batches_y = train_y[:batch_size*num_batches].view(num_batches,batch_size, train_y.shape[1])\n",
    "\n",
    "    # batches = torch.cat((batches_X, batches_y), -1)\n",
    "    # print(\"Shape of batches full of X inst: \", batches_X.shape)\n",
    "    # print(\"Shape of batches full of y inst: \", batches_y.shape)\n",
    "    return batches_X, batches_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc547e2a-648e-46dd-92e4-6f35b7e66621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Average loss after epoch 1: 111.6510002625263\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Average loss after epoch 2: 45.17123385837011\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Average loss after epoch 3: 23.7831861356945\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Average loss after epoch 4: 15.608368954761788\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Average loss after epoch 5: 11.644263831874751\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Average loss after epoch 6: 9.87129383909104\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Average loss after epoch 7: 8.698806805074062\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Average loss after epoch 8: 8.140867290127536\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Average loss after epoch 9: 7.39918151096729\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Average loss after epoch 10: 7.678291720816928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaggerModel(\n",
       "  (embed): Embedding(19674, 100)\n",
       "  (rnn): RNN(100, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "torch.manual_seed(42)\n",
    "DIM_EMBEDDING = 100\n",
    "RNN_HIDDEN = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "\n",
    "class TaggerModel(torch.nn.Module):\n",
    "    def __init__(self, nwords, ntags):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        self.embed = nn.Embedding(nwords, DIM_EMBEDDING)\n",
    "        self.rnn = nn.RNN(DIM_EMBEDDING, RNN_HIDDEN, batch_first=True)\n",
    "        self.fc = nn.Linear(RNN_HIDDEN, ntags)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        word_vectors = self.embed(input_data)\n",
    "        output, hidden = self.rnn(word_vectors)\n",
    "        predictions = self.fc(output)\n",
    "\n",
    "        return predictions \n",
    "\n",
    "model = TaggerModel(len(idx2word), len(idx2label))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "#creating the batches \n",
    "batches_X, batches_y = create_batches(BATCH_SIZE, train_X, train_y)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    # reset the gradient\n",
    "    model.zero_grad()\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    loss_sum = 0\n",
    "\n",
    "    # loop over batches\n",
    "    for X, y in zip(batches_X, batches_y):\n",
    "        predicted_values = model.forward(X)\n",
    "        predicted_values=predicted_values.view(BATCH_SIZE*max_len, -1) #resizing tensor to 2D from 3D\n",
    "        \n",
    "        # calculate loss\n",
    "        y=torch.flatten(y.view(BATCH_SIZE*max_len, -1)) #flattening to make it 1D\n",
    "        loss = loss_function(predicted_values, y)\n",
    "        loss_sum += loss.item() #avg later\n",
    "\n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Average loss after epoch {epoch+1}: {loss_sum/batches_X.shape[0]}\")\n",
    "        \n",
    "# set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8bd1d-0165-4105-8f12-4a0f33063bd0",
   "metadata": {},
   "source": [
    "## RNN eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04983a4b-756b-40a1-9a2b-0e6b03414c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test set: 0.9564197383593781\n"
     ]
    }
   ],
   "source": [
    "#Evaluating on test data we will predict using trained TaggerModel\n",
    "predictions_test = model.forward(test_X)\n",
    "#gives probabilities for each tag (dim=18) for each word/feature (dim=159) for each sentence(dim=2000)\n",
    "#we want to classify each word for the part-of-speech with highest probability\n",
    "\n",
    "labels_test = torch.argmax(predictions_test, 2)\n",
    "labels_test = torch.flatten(labels_test) #model predictions\n",
    "test_y_flat = torch.flatten(test_y) #true labels\n",
    "\n",
    "acc = []\n",
    "for i in range(len(labels_test)):\n",
    "    if test_y_flat[i]!=0:\n",
    "        acc.append(int(labels_test[i]==test_y_flat[i]))\n",
    "\n",
    "accuracy = sum(acc)/len(acc)\n",
    "print(f\"Model accuracy on test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f446c5-3aa1-4714-a718-0c5c1da37288",
   "metadata": {},
   "source": [
    "## **Step 2.2:** Implement BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253446e-0a9d-47dc-ac95-b88711d40e7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2.1: Testing MASK outputs with AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f43c6e-bd35-473f-9fa7-5140729e6635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mother']\n",
      "['Denmark']\n",
      "['kill']\n",
      "['Denmark']\n",
      "['port']\n",
      "['Denmark']\n",
      "['Denmark']\n",
      "['mom']\n",
      "['agriculture']\n",
      "['agriculture']\n",
      "['married']\n",
      "['dead']\n",
      "['identical']\n",
      "['gone']\n",
      "['His']\n",
      "['He']\n",
      "['Whoever']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokzr = AutoTokenizer.from_pretrained('bert-base-multilingual-cased', use_fast=False)\n",
    "\n",
    "def getTopN(inputSent, model, tokzr, topn=1):\n",
    "    maskId = tokzr.convert_tokens_to_ids(tokzr.mask_token)\n",
    "    tokenIds = tokzr(inputSent).input_ids\n",
    "    if maskId not in tokenIds:\n",
    "        return 'please include ' + tokzr.mask_token + ' in your input'\n",
    "    maskIndex = tokenIds.index(maskId)\n",
    "    logits = model(torch.tensor([tokenIds])).logits\n",
    "    return tokzr.convert_ids_to_tokens(torch.topk(logits, topn, dim=2).indices[0][maskIndex])\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')\n",
    "tokzr = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "getTopN('This is a [MASK] test.', model, tokzr, 5)\n",
    "print(getTopN('daughter is to dad as son is to [MASK].', model, tokzr))\n",
    "print(getTopN('Małopolska is to Poland as Sjælland is to [MASK].', model, tokzr))\n",
    "print(getTopN('robić is to robienie as palić is to [MASK].', model, tokzr))\n",
    "print(getTopN('Cracow is to Poland as Copenhagen is to [MASK].', model, tokzr))\n",
    "print(getTopN('cracow is to poland as copenhagen is to [MASK].', model, tokzr))\n",
    "print(getTopN('Maiopolska is to Poland as Sjælland is to [MASK].', model, tokzr)) #typo\n",
    "print(getTopN('Małopolska is to Poland as Sjælland is to [MASK]:', model, tokzr))\n",
    "print(getTopN('doughter is to dad as san is to [MASK].', model, tokzr))\n",
    "print(getTopN('The woman occupation is [MASK].', model, tokzr))\n",
    "print(getTopN('The man occupation is [MASK].', model, tokzr))\n",
    "print(getTopN('The Polish lady is [MASK].', model, tokzr))\n",
    "print(getTopN('The Danish man is [MASK].', model, tokzr))\n",
    "print(getTopN('The young girls outfit is [MASK].', model, tokzr))\n",
    "print(getTopN('The old womans outfit is [MASK].', model, tokzr))\n",
    "print(getTopN('[MASK] occupation is teacher.', model, tokzr))\n",
    "print(getTopN('[MASK] does not have a job', model, tokzr))\n",
    "print(getTopN('[MASK] earns money', model, tokzr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5a6b6-2421-4983-9a48-5487ac8d0b71",
   "metadata": {},
   "source": [
    "## 2.2.2: bert-topic.py code, needs to be run as .py on HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb06d0d-97ba-4f93-9e40-4c0d775a870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A basic classifier based on the transformers (https://github.com/huggingface/transformers) \n",
    "library. It loads a masked language model (by default distilbert), and adds a linear layer for\n",
    "prediction. Usage with ITU HPC:\n",
    "\n",
    "python3 bert-topic.py Project_description/en_ewt-ud-train.iob2 Project_description/en_ewt-ud-dev.iob2\n",
    "\"\"\"\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import codecs\n",
    "import sys\n",
    "import myutils\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# set seed for consistency\n",
    "torch.manual_seed(42)\n",
    "# Set some constants\n",
    "MLM = 'distilbert-base-cased'\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 3\n",
    "# We have an UNK label for robustness purposes, it makes it easier to run on\n",
    "# data with other labels, or without labels.\n",
    "UNK = \"[UNK]\"\n",
    "MAX_TRAIN_SENTS=64\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class ClassModel(torch.nn.Module):\n",
    "    def __init__(self, nlabels: int, mlm: str):\n",
    "        \"\"\"\n",
    "        Model for classification with transformers.\n",
    "\n",
    "        The architecture of this model is simple, we just have a transformer\n",
    "        based language model, and add one linear layer to converts it output\n",
    "        to our prediction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlabels : int\n",
    "            Vocabulary size of output space (i.e. number of labels)\n",
    "        mlm : str\n",
    "            Name of the transformers language model to use, can be found on:\n",
    "            https://huggingface.co/models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The transformer model to use\n",
    "        self.mlm = AutoModel.from_pretrained(mlm)\n",
    "\n",
    "        # Find the size of the output of the masked language model\n",
    "        if hasattr(self.mlm.config, 'hidden_size'):\n",
    "            self.mlm_out_size = self.mlm.config.hidden_size\n",
    "        elif hasattr(self.mlm.config, 'dim'):\n",
    "            self.mlm_out_size = self.mlm.config.dim\n",
    "        else: # if not found, guess\n",
    "            self.mlm_out_size = 768\n",
    "\n",
    "        # Create prediction layer\n",
    "        self.hidden_to_label = torch.nn.Linear(self.mlm_out_size, nlabels)\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : torch.tensor\n",
    "            Tensor with wordpiece indices. shape=(batch_size, max_sent_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output_scores : torch.tensor\n",
    "            ?. shape=(?,?)\n",
    "        \"\"\"\n",
    "        # Run transformer model on input\n",
    "        mlm_out = self.mlm(input)\n",
    "\n",
    "        # Keep only the last layer: shape=(batch_size, max_len, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out.last_hidden_state\n",
    "        # Keep only the output for the first ([CLS]) token: shape=(batch_size, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out[:,:1,:].squeeze()\n",
    "\n",
    "        # Matrix multiply to get scores for each label: shape=(?,?)\n",
    "        output_scores = self.hidden_to_label(mlm_out)\n",
    "\n",
    "        return output_scores\n",
    "\n",
    "    def run_eval(self, text_batched: List[torch.tensor], labels_batched: List[torch.tensor]):\n",
    "        \"\"\"\n",
    "        Run evaluation: predict and score\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_batched : List[torch.tensor]\n",
    "            list with batches of text, containing wordpiece indices.\n",
    "        labels_batched : List[torch.tensor]\n",
    "            list with batches of labels (converted to ints).\n",
    "        model : torch.nn.module\n",
    "            The model to use for prediction.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            accuracy of model on labels_batches given feats_batches\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for sents, labels in zip(text_batched, labels_batched):\n",
    "            output_scores = self.forward(sents)\n",
    "            pred_labels = torch.argmax(output_scores, 1)\n",
    "            for gold_label, pred_label in zip(labels, pred_labels):\n",
    "                total += 1\n",
    "                if gold_label.item() == pred_label.item():\n",
    "                    match+= 1\n",
    "        return(match/total)\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    print('Please provide path to training and development data')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('reading data...')\n",
    "    train_text, train_labels = myutils.read_data(sys.argv[1])\n",
    "    train_text = train_text[:MAX_TRAIN_SENTS]\n",
    "    train_labels = train_labels[:MAX_TRAIN_SENTS]\n",
    "    \n",
    "    id2label, label2id = myutils.labels2lookup(train_labels, UNK)\n",
    "    NLABELS = len(id2label)\n",
    "    print(train_labels)\n",
    "    print(label2id)\n",
    "    train_labels = [label2id[label] for label in train_labels]\n",
    "    \n",
    "    dev_text, dev_labels = myutils.read_data(sys.argv[2])\n",
    "    dev_labels = [label2id[label] for label in dev_labels]\n",
    "    \n",
    "    print('tokenizing...')\n",
    "    tokzr = AutoTokenizer.from_pretrained(MLM)\n",
    "    train_tokked = myutils.tok(train_text, tokzr)\n",
    "    dev_tokked = myutils.tok(dev_text, tokzr)\n",
    "    PAD = tokzr.pad_token_id\n",
    "    \n",
    "    print('converting to batches...')\n",
    "    train_text_batched, train_labels_batched = myutils.to_batch(train_tokked, train_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "    # Note, some data is trown away if len(text_tokked)%BATCH_SIZE!= 0\n",
    "    dev_text_batched, dev_labels_batched = myutils.to_batch(dev_tokked, dev_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "    \n",
    "    print('initializing model...')\n",
    "    model = ClassModel(NLABELS, MLM)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    \n",
    "    print('training...')\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('=====================')\n",
    "        print('starting epoch ' + str(epoch))\n",
    "        model.train() \n",
    "    \n",
    "        # Loop over batches\n",
    "        loss = 0\n",
    "        for batch_idx in range(0, len(train_text_batched)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output_scores = model.forward(train_text_batched[batch_idx])\n",
    "            batch_loss = loss_function(output_scores, train_labels_batched[batch_idx])\n",
    "            loss += batch_loss.item()\n",
    "    \n",
    "            batch_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "    \n",
    "        dev_score = model.run_eval(dev_text_batched, dev_labels_batched)\n",
    "        print('Loss: {:.2f}'.format(loss))\n",
    "        print('Acc(dev): {:.2f}'.format(100*dev_score))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421f120-f58f-4872-b500-da7d376e3747",
   "metadata": {},
   "source": [
    "## BERT eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3bfc4-08f1-4427-a4c0-9702dca9a3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
