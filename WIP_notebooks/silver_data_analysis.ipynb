{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddd75fe-c6c1-4536-9d76-884c8409618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ce8df-e7c9-4276-a376-f5811124241e",
   "metadata": {},
   "source": [
    "# Reading the results in from the manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4f6a25-3d0e-4657-b800-f91d004b1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.read_csv('../data-files_and_results/annotations_Results.csv', sep=';', encoding='utf-8')\n",
    "with open('../data-files_and_results/subset_of_wikiann.pkl', 'rb') as f:\n",
    "    silver_data=dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d154ff-b8a5-49ce-9a7b-761605fbdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=results.fillna('0')\n",
    "results['labels1']=results['labels1'].astype(int)\n",
    "results['labels2']=results['labels2'].astype(int)\n",
    "results['labels1']=results['labels1'].astype(str)\n",
    "results['labels2']=results['labels2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5454458a-f072-4cb6-9b9d-33e255cf408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=''\n",
    "YAY=0\n",
    "FALSEPOSNEG=0\n",
    "ORGLOCERR=0\n",
    "LOCSPLITERR=0\n",
    "locspliterrlist=[]\n",
    "OTHERERR=0\n",
    "weird_results=[]\n",
    "annotator1_results=[]\n",
    "annotator2_results=[]\n",
    "\n",
    "location=False\n",
    "for index, row in results.iterrows():\n",
    "    \n",
    "    token=row.iloc[0]\n",
    "    label1=row.iloc[1]\n",
    "    label2=row.iloc[2]\n",
    "    \n",
    "    if token[0]=='#':\n",
    "        sentence=token[17:]\n",
    "        continue\n",
    "    annotator1_results.append(label1)\n",
    "    annotator2_results.append(label2)\n",
    "    \n",
    "    if label1 == label2:\n",
    "        YAY +=1\n",
    "        \n",
    "    else:\n",
    "        if ( ( label1 in ['3','4'] ) and (label2 in ['5','6'] ) ) or ( ( label2 in ['3','4'] ) and (label1 in ['5','6'] ) ):\n",
    "            ORGLOCERR+=1\n",
    "\n",
    "        elif label1=='5' or label2=='5':\n",
    "            location=True\n",
    "            if label1=='0' or label2=='0':\n",
    "                FALSEPOSNEG+=1\n",
    "                continue\n",
    "                \n",
    "        elif ( label1=='0' and label2 in ['5','6'] ) or ( label2=='0' and label1 in ['5','6'] ) or (label1 in ['5', '6'] and label2 in ['5', '6']):\n",
    "            LOCSPLITERR+=1\n",
    "            locspliterrlist.append((sentence, token, label1, label2))\n",
    "\n",
    "        elif label1=='0' or label2=='0':\n",
    "            FALSEPOSNEG+=1\n",
    "            #print(sentence, token, label1, label2, \"\\n\")\n",
    "        else:\n",
    "            OTHERERR+=1\n",
    "            weird_results.append((sentence, token, label1, label2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7303e450-f410-4303-bc16-fc39e7a654c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct results:  14260\n",
      "False positives or negatives:  412\n",
      "Disagreements of organisation or location:  317\n",
      "Location-splitting error (or perhaps just false negatives):  376\n",
      "Nr of other unidentified errors:  81\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct results: \", YAY)\n",
    "print(\"False positives or negatives: \", FALSEPOSNEG)\n",
    "print(\"Disagreements of organisation or location: \", ORGLOCERR)\n",
    "print(\"Location-splitting error (or perhaps just false negatives): \", LOCSPLITERR)\n",
    "print(\"Nr of other unidentified errors: \", OTHERERR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62163f4-1f17-43a1-a5d5-d352062ccc97",
   "metadata": {},
   "source": [
    "# Accuracy and Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb1f796-07ef-469d-a47d-910fdd188314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.917075647317651\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate accuracy between two lists of labels\n",
    "def calculate_accuracy(labels1, labels2):\n",
    "    total_tokens = len(labels1)\n",
    "    correct_tokens = sum(1 for label1, label2 in zip(labels1, labels2) if label1 == label2)\n",
    "    accuracy = correct_tokens / total_tokens\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(results['labels1'], results['labels2'])\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960816a1-18a6-4324-be29-7f19f8d4c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.8371968832019276\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def cohen_kappa(annotation1, annotation2):\n",
    "    #how much of each annotation\n",
    "    count1 = Counter(annotation1)\n",
    "    count2 = Counter(annotation2)\n",
    "    \n",
    "    # Observed agreement (P_o)\n",
    "    observed_agreement = sum((a == b) for a, b in zip(annotation1, annotation2)) / len(annotation1)\n",
    "    \n",
    "    # Expected agreement (P_e)\n",
    "    total = len(annotation1)\n",
    "    categories = set(annotation1).union(set(annotation2)) # categories that appear in either of annotations\n",
    "    expected_agreement = 0\n",
    "    \n",
    "    for category in categories:\n",
    "        p1 = count1.get(category, 0) / total\n",
    "        p2 = count2.get(category, 0) / total\n",
    "        expected_agreement += p1 * p2\n",
    "    \n",
    "    # Cohen's Kappa calculation\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement) #from the formula\n",
    "    return kappa\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa = cohen_kappa(results['labels1'], results['labels2'])\n",
    "print(f\"Cohen's Kappa: {kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff3d4b-4874-40b2-9516-63f4c7172b3a",
   "metadata": {},
   "source": [
    "To Interpret Kappa Score:\n",
    "- < 0.2: Poor agreement\n",
    "- 0.2 - 0.4: Fair\n",
    "- 0.4 - 0.6: Moderate\n",
    "- 0.6 - 0.8: Substantial\n",
    "- 0.8 - 1.0: Near-perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad31ec-b855-4d77-864b-66c0d5b44779",
   "metadata": {},
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c86b7f4-5ee8-4209-9a86-f1844eb36035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 (Annotator1 vs Annotator2): 0.8267109915805377\n",
      "Micro F1 (Annotator1 vs Annotator2): 0.9067209257964011\n",
      "Weighted F1 (Annotator1 vs Annotator2): 0.9057871556892688\n",
      "Per-class F1 (Annotator1 vs Annotator2): [0.96136497 0.94453419 0.94872938 0.75312856 0.79926335 0.75877793\n",
      " 0.62117856]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "gold_a = annotator1_results  # labels1\n",
    "gold_b = annotator2_results  # labels2\n",
    "\n",
    "y_true = [int(tag) for tag in gold_a]\n",
    "y_pred = [int(tag) for tag in gold_b]\n",
    "\n",
    "print(\"Macro F1 (Annotator1 vs Annotator2):\", f1_score(y_true, y_pred, average='macro'))\n",
    "print(\"Micro F1 (Annotator1 vs Annotator2):\", f1_score(y_true, y_pred, average='micro'))\n",
    "print(\"Weighted F1 (Annotator1 vs Annotator2):\", f1_score(y_true, y_pred, average='weighted'))\n",
    "print(\"Per-class F1 (Annotator1 vs Annotator2):\", f1_score(y_true, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9896b-87f5-4256-b6eb-eefc12470247",
   "metadata": {},
   "source": [
    "# Silver vs Gold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e837a9bd-6fdc-4674-ae7e-f0a4de23a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading silver data\n",
    "silver_data_list=[]\n",
    "for index, row in silver_data.iterrows():\n",
    "    tokens=row.iloc[0]\n",
    "    tags=row.iloc[1]\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        tag=str(tag)\n",
    "        silver_data_list.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fd7bd-7b9c-45ad-8d51-7462636873b0",
   "metadata": {},
   "source": [
    "## Error types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5032c87c-d85f-4994-a931-5ecc1e977ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Error Type Counts ---\n",
      "Non-critical: Subtype or disagreement: 822\n",
      "Correct: 12517\n",
      "Critical: False positive: 2077\n",
      "Critical: PER↔ORG/LOC confusion: 75\n",
      "Critical: Missed entity: 80\n",
      "Critical: ORG/LOC↔PER confusion: 156\n",
      "\n",
      "--- One Example per Error Type ---\n",
      "\n",
      "Non-critical: Subtype or disagreement\n",
      "Index: 1\n",
      "Token: R.H.\n",
      "Silver: 3 | Annotator1: 5 | Annotator2: 5\n",
      "\n",
      "Critical: False positive\n",
      "Index: 41\n",
      "Token: List\n",
      "Silver: 3 | Annotator1: 0 | Annotator2: 0\n",
      "\n",
      "Critical: PER↔ORG/LOC confusion\n",
      "Index: 511\n",
      "Token: Exeter\n",
      "Silver: 2 | Annotator1: 3 | Annotator2: 3\n",
      "\n",
      "Critical: Missed entity\n",
      "Index: 595\n",
      "Token: Meyrick\n",
      "Silver: 0 | Annotator1: 1 | Annotator2: 1\n",
      "\n",
      "Critical: ORG/LOC↔PER confusion\n",
      "Index: 626\n",
      "Token: Brian\n",
      "Silver: 3 | Annotator1: 1 | Annotator2: 1\n"
     ]
    }
   ],
   "source": [
    "def classify_error(silver, label1, label2):\n",
    "    try:\n",
    "        silver = int(silver)\n",
    "        label1 = int(label1)\n",
    "        label2 = int(label2)\n",
    "    except ValueError:\n",
    "        return \"Invalid label\"\n",
    "\n",
    "    def get_type(label):\n",
    "        if label in [1, 2]:\n",
    "            return \"PER\"\n",
    "        elif label in [3, 4]:\n",
    "            return \"ORG\"\n",
    "        elif label in [5, 6]:\n",
    "            return \"LOC\"\n",
    "        else:\n",
    "            return \"O\"\n",
    "\n",
    "    silver_type = get_type(silver)\n",
    "    gold1_type = get_type(label1)\n",
    "    gold2_type = get_type(label2)\n",
    "\n",
    "    if silver == label1 or silver == label2:\n",
    "        return \"Correct\"\n",
    "    if silver_type == \"O\" and (gold1_type != \"O\" or gold2_type != \"O\"):\n",
    "        return \"Critical: Missed entity\"\n",
    "    if silver_type != \"O\" and gold1_type == \"O\" and gold2_type == \"O\":\n",
    "        return \"Critical: False positive\"\n",
    "    if silver_type == \"PER\" and (gold1_type in {\"ORG\", \"LOC\"} or gold2_type in {\"ORG\", \"LOC\"}):\n",
    "        return \"Critical: PER↔ORG/LOC confusion\"\n",
    "    if silver_type in {\"ORG\", \"LOC\"} and (gold1_type == \"PER\" or gold2_type == \"PER\"):\n",
    "        return \"Critical: ORG/LOC↔PER confusion\"\n",
    "    return \"Non-critical: Subtype or disagreement\"\n",
    "\n",
    "# Count and track examples\n",
    "error_counts = Counter()\n",
    "example_errors = {}\n",
    "\n",
    "silver_data_index = 0\n",
    "\n",
    "for index, row in results.iterrows():\n",
    "    token = row.iloc[0]\n",
    "\n",
    "    if isinstance(token, str) and token.strip().startswith(\"#\"):\n",
    "        continue\n",
    "\n",
    "    label1 = row.iloc[1]\n",
    "    label2 = row.iloc[2]\n",
    "\n",
    "    if silver_data_index >= len(silver_data_list):\n",
    "        break\n",
    "\n",
    "    silver_label = silver_data_list[silver_data_index]\n",
    "    error_type = classify_error(silver_label, label1, label2)\n",
    "\n",
    "    error_counts[error_type] += 1\n",
    "\n",
    "    if error_type != \"Correct\" and error_type not in example_errors:\n",
    "        example_errors[error_type] = {\n",
    "            \"Index\": index,\n",
    "            \"Token\": token,\n",
    "            \"Silver\": silver_label,\n",
    "            \"Annotator1\": label1,\n",
    "            \"Annotator2\": label2\n",
    "        }\n",
    "\n",
    "    silver_data_index += 1\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n--- Error Type Counts ---\")\n",
    "for etype, count in error_counts.items():\n",
    "    print(f\"{etype}: {count}\")\n",
    "\n",
    "print(\"\\n--- One Example per Error Type ---\")\n",
    "for etype, example in example_errors.items():\n",
    "    print(f\"\\n{etype}\")\n",
    "    print(f\"Index: {example['Index']}\")\n",
    "    print(f\"Token: {example['Token']}\")\n",
    "    print(f\"Silver: {example['Silver']} | Annotator1: {example['Annotator1']} | Annotator2: {example['Annotator2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae039588-e6c5-4edf-9d93-44720d22f5db",
   "metadata": {},
   "source": [
    "# Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "891a95b8-38f2-4d26-8b03-1ed6288e59c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7579\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy2(predictions, references):\n",
    "    correct = sum(p == r for p, r in zip(predictions, references))\n",
    "    return correct / len(predictions)\n",
    "\n",
    "accuracy = calculate_accuracy2(silver_data_list, annotator1_results)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773c600-7c81-4060-ad01-04c6e7a0335d",
   "metadata": {},
   "source": [
    "# F1 score, Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "078f8c6c-dbf2-4cf1-a941-c95128475af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (len(silver_data_list) == len(annotator1_results) == len(annotator2_results)):\n",
    "    print(\"⚠️ Warning: Your lists are not the same length.\")\n",
    "    print(f\"silver: {len(silver)}, annotator1: {len(gold_a)}, annotator2: {len(gold_b)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70cdd7d0-35b7-420f-958e-8500d3a6172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.7225173517266948\n",
      "Micro F1: 0.7958924143193234\n",
      "Weighted F1: 0.8179375371524854\n",
      "Per-class F1: [0.87502765 0.89385475 0.83951581 0.54598117 0.5036706  0.74724173\n",
      " 0.65232975]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    " \n",
    "silver = silver_data_list\n",
    "gold_a = annotator1_results\n",
    "gold_b = annotator2_results\n",
    " \n",
    "y_true_relaxed = []\n",
    "y_pred_relaxed = []\n",
    " \n",
    "for i, pred in enumerate(silver):\n",
    "    if pred == gold_a[i] or pred == gold_b[i]:\n",
    "        y_true_relaxed.append(pred)  # it's a match, count it as correct\n",
    "        y_pred_relaxed.append(pred)\n",
    "    else:\n",
    "        y_true_relaxed.append(gold_a[i])  # mark mismatch with one of the annotators\n",
    "        y_pred_relaxed.append(pred)\n",
    " \n",
    "print(\"Macro F1:\", f1_score(y_true_relaxed, y_pred_relaxed, average='macro'))\n",
    "print(\"Micro F1:\", f1_score(y_true_relaxed, y_pred_relaxed, average='micro')) # good for imbalanced classes\n",
    "print(\"Weighted F1:\", f1_score(y_true_relaxed, y_pred_relaxed, average='weighted'))\n",
    "print(\"Per-class F1:\", f1_score(y_true_relaxed, y_pred_relaxed, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d24bc2f-acd9-46e8-aa08-cc2691c8179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (macro): 0.6905649349444553\n",
      "Recall (macro): 0.813807712365085\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision (macro):\", precision_score(y_true_relaxed, y_pred_relaxed, average='macro'))\n",
    "print(\"Recall (macro):\", recall_score(y_true_relaxed, y_pred_relaxed, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a903f2c-f33f-43e4-afe4-39bd733e43e0",
   "metadata": {},
   "source": [
    "# Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ac049-7aaf-4ff5-b63b-192d9954b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_kappa(annotation1, annotation2):\n",
    "    #how much of each annotation\n",
    "    count1 = Counter(annotation1)\n",
    "    count2 = Counter(annotation2)\n",
    "    \n",
    "    # Observed agreement (P_o)\n",
    "    observed_agreement = sum((a == b) for a, b in zip(annotation1, annotation2)) / len(annotation1)\n",
    "    \n",
    "    # Expected agreement (P_e)\n",
    "    total = len(annotation1)\n",
    "    categories = set(annotation1).union(set(annotation2)) # categories that appear in either of annotations\n",
    "    expected_agreement = 0\n",
    "    \n",
    "    for category in categories:\n",
    "        p1 = count1.get(category, 0) / total\n",
    "        p2 = count2.get(category, 0) / total\n",
    "        expected_agreement += p1 * p2\n",
    "    \n",
    "    # Cohen's Kappa calculation\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement) #from the formula\n",
    "    return kappa\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa1 = cohen_kappa(silver, gold_a)\n",
    "kappa2 = cohen_kappa(silver, gold_b)\n",
    "kappa_avg = (kappa1+kappa2)/2\n",
    "print(f\"Cohen's Kappa between our annotation and the silver data: {kappa_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5b4f3a-d4d7-45b8-9ed5-a25924c524b8",
   "metadata": {},
   "source": [
    "# Comparing Gold and Silver data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ee214-afda-4823-92c5-0e064a672aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (pred, gold1, gold2) in enumerate(zip(silver_data_list, annotator1_results, annotator2_results)):\n",
    "    if pred != gold1 and pred != gold2:\n",
    "        print(f\"Index {i}: Silver = {pred}, Annotator 1 = {gold1}, Annotator 2 = {gold2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c1d79-ccdb-4dba-8e83-0af504aef2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load results\n",
    "df = pd.read_csv(\"results.csv\", sep=\";\")  # Try semicolon\n",
    "\n",
    "\n",
    "# Count occurrences of each class in gold and silver\n",
    "gold_counts = df['labels1'].value_counts().sort_index()\n",
    "silver_counts = df['labels2'].value_counts().sort_index()\n",
    "\n",
    "# Combine into a DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Gold': gold_counts,\n",
    "    'Silver': silver_counts\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "# Plot side-by-side bar chart\n",
    "ax = comparison_df.plot(kind='bar', figsize=(6, 6), width=0.8)\n",
    "plt.title(\"Class Distribution: Gold vs Silver\")\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"class_distribution.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccbf3fe-41c4-42a9-b6c5-8c960319848e",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853952f-7f0d-4d70-b300-33acc3a87972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes labels1 = gold, labels2 = silver\n",
    "df = pd.read_csv(\"results.csv\", sep=\";\")  # Or your working version\n",
    "\n",
    "# Missed ORG\n",
    "missed_org = df[(df['labels1'] == 'ORG') & (df['labels2'] != 'ORG')]\n",
    "\n",
    "# Hallucinated ORG (predicted ORG where it shouldn't be)\n",
    "hallucinated_org = df[(df['labels1'] != 'ORG') & (df['labels2'] == 'ORG')]\n",
    "\n",
    "# Misclassified ORG (could be either direction)\n",
    "misclassified_org = df[(df['labels1'] == 'ORG') & (df['labels2'] != 'ORG')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53450e8-e12b-4911-b1a8-d50adbe5fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing labels\n",
    "df_clean = df.dropna(subset=['labels1', 'labels2'])\n",
    "\n",
    "# Extract gold and silver labels\n",
    "y_true = df_clean['labels1']\n",
    "y_pred = df_clean['labels2']\n",
    "\n",
    "# Get consistent set of labels\n",
    "labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "# Now safely compute and display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(xticks_rotation=45, cmap='Reds')\n",
    "plt.title(\"Confusion Matrix: Silver vs Gold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9f869-5ee4-4428-9bea-4cca3e5bcedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
